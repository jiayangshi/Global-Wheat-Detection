{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is directly inspired from Unet starter notebook by Paul Dnt(https://www.kaggle.com/pednt9) .The Goal of this notebook is to build a simple Unet baseline model using Keras "},{"metadata":{},"cell_type":"markdown","source":"Cf. https://arxiv.org/pdf/1505.04597.pdf"},{"metadata":{},"cell_type":"markdown","source":"Inspired from https://github.com/jocicmarko/ultrasound-nerve-segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.measure import label, regionprops\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport imageio\nimport cv2 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Credits: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu/"},{"metadata":{},"cell_type":"markdown","source":"## Use TPU, optimal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Detect hardware, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SETTINGS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set some parameters\nIMG_WIDTH = 1024\nIMG_HEIGHT = 1024\nBATCH_SIZE = 1\nEPOCHS = 25\n\n#multi processing setting for model\nmax_queue_size=4 \nworkers=4 \nuse_multiprocessing=False\n\nTRAIN_PATH = '/kaggle/input/global-wheat-detection/train/'\nTEST_PATH = '/kaggle/input/global-wheat-detection/test/'\nSC_FACTOR = int(1024 / IMG_WIDTH)\n\nwarnings.filterwarnings('ignore')\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/global-wheat-detection/\"\ntrain_folder = os.path.join(PATH, \"train\")\ntest_folder = os.path.join(PATH, \"test\")\n\ntrain_csv_path = os.path.join(PATH, \"train.csv\")\ndf = pd.read_csv(train_csv_path)\nsample_sub = pd.read_csv(PATH + \"sample_submission.csv\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train and test IDs and paths\ntrain_ids = os.listdir(TRAIN_PATH)\ntest_ids = os.listdir(TEST_PATH)\n\n# train_ids = train_ids[0:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def append_path(path):\n    return np.vectorize(lambda file: os.path.join(path, file))\n\ntrain_paths = append_path('/kaggle/input/global-wheat-detection/train')(train_ids[:])\ntest_paths = append_path('/kaggle/input/global-wheat-detection/test')(test_ids[:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_paths","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Produce the labels, once done, no moer needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_polygon(coords):\n    xm, ym, w, h = coords\n    xm, ym, w, h = xm / SC_FACTOR, ym / SC_FACTOR, w / SC_FACTOR, h / SC_FACTOR   # scale values if image was downsized\n    return [(xm, ym), (xm, ym + h), (xm + w, ym + h), (xm + w, ym)]\n\n# masks = dict() # dictionnary containing all masks\n\n# for img_id, gp in tqdm(df.groupby(\"image_id\")):\n#     gp['polygons'] = gp['bbox'].apply(eval).apply(lambda x: make_polygon(x))\n\n#     img = Image.new('L', (IMG_WIDTH, IMG_HEIGHT), 0)\n#     for pol in gp['polygons'].values:\n#         ImageDraw.Draw(img).polygon(pol, outline=1, fill=1)\n\n#     mask = np.array(img, dtype=np.uint8)\n#     masks[img_id] = mask\n\n# for i,train_id in enumerate(train_ids):\n#     if train_id.split('.')[0] not in masks.keys():\n#         masks[train_id.split('.')[0]] = np.zeros((IMG_WIDTH, IMG_HEIGHT), dtype=np.uint8)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.imshow(masks['ffbf75e5b'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x=imageio.imread('/kaggle/input/global-wheat-detection/train/00333207f.jpg')\n# x=x/255.\n# x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_labels = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n# train_datas = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH,3), dtype=np.float32)\n\n# for i,train_id in enumerate(train_ids):\n#     train_labels[i] = masks[train_id.split('.')[0]]\n#     train_datas[i] = imageio.imread('/kaggle/input/global-wheat-detection/train/'+train_id)\n#     train_datas[i] = train_datas[i]/255.\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output_dir = '/kaggle/working'\n# #os.makedirs(output_dir)\n\n# for key, value in masks.items():\n#     imageio.imwrite(output_dir+'/'+key+'.jpg',value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n#     train_paths, train_labels, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None):\n    image_size=(IMG_WIDTH, IMG_HEIGHT)\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_image(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n#     img = Image.new('L', (IMG_WIDTH, IMG_HEIGHT), 0)\n#     imgname = filename.split('/')[-1]\n#     imgname = imgname.split('.')[0]\n#     for x in df.loc[lambda df: df['image_id'] == imgname]['bbox']:\n#         x = x[1:-1].split(',')\n#         x = [float(ix) for ix in x]\n#         ImageDraw.Draw(img).polygon(make_polygon(x), outline=1, fill=1)\n#     mask = np.array(img, dtype=np.uint8)\n#     label = mask\n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((train_paths,train_labels))\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .cache()\n#     .repeat()\n#     .shuffle(1024)\n#     .batch(BATCH_SIZE)\n#     .prefetch(AUTO)\n# )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valid_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices((valid_paths, valid_labels))\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .batch(BATCH_SIZE)\n#     .cache()\n#     .prefetch(AUTO)\n# )\n\n# test_dataset = (\n#     tf.data.Dataset\n#     .from_tensor_slices(test_paths)\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .batch(BATCH_SIZE)\n# )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Self Defined Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, batch_size=8, dim=(1024,1024),n_channels=3):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.indexes = np.arange(len(self.list_IDs))\n\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n    \n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels),dtype=np.float32)\n        y = np.empty((self.batch_size, *self.dim, 1), dtype=np.uint8)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            X[i] = imageio.imread('/kaggle/input/global-wheat-detection/train/'+ID)\n            X[i] = X[i]/255.\n\n            # Store class\n            y[i,:,:,0] = imageio.imread('/kaggle/input/labels/labels/'+ID)\n            y[i,:,:,0] = y[i,:,:,0]/255.\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids = train_ids[:int(len(train_ids)*0.85)]\nvalid_ids = train_ids[int(len(train_ids)*0.85):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = DataGenerator(list_IDs=train_ids,batch_size=BATCH_SIZE, dim=(IMG_WIDTH, IMG_HEIGHT),n_channels=3)\nvalid_gen = DataGenerator(list_IDs=valid_ids,batch_size=BATCH_SIZE, dim=(IMG_WIDTH, IMG_HEIGHT),n_channels=3)\ntest_imgs = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH,3), dtype=np.float32)\n\n\nfor i,test_id in enumerate(test_ids):\n    test_imgs[i] = imageio.imread('/kaggle/input/global-wheat-detection/test/'+test_id)\n    test_imgs[i] = test_imgs[i]/255.\n\ntest_gen = test_imgs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's plot a quick example of the image and mask of Generator: "},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = train_gen.__getitem__(0)\nplt.imshow(x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# im = Image.fromarray(masks[list(masks.keys())[7]])\n# plt.imshow(im)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's load the dataset"},{"metadata":{},"cell_type":"markdown","source":"Let's quickly look at what the images and mask look like (credits : https://www.kaggle.com/devvindan/wheat-detection-eda)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_images(images, num=2):\n    \n    images_to_show = np.random.choice(images, num)\n\n    for image_id in images_to_show:\n\n        image_path = os.path.join(train_folder, image_id + \".jpg\")\n        image = Image.open(image_path)\n\n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in df[df['image_id'] == image_id]['bbox']]\n\n        # visualize them\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n\n        plt.figure(figsize = (15,15))\n        plt.imshow(image)\n        plt.show()\n\n\nunique_images = df['image_id'].unique()\nshow_images(unique_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Credits to : https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63044\n\ndef castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n    intersection = true * pred\n    notTrue = 1 - true\n    union = true + (notTrue * pred)\n\n    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n\ndef competitionMetric2(true, pred):\n\n    tresholds = [0.5 + (i * 0.05)  for i in range(5)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) / castF(K.shape(true)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Build U-Net model\n    \ninputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\ns = Lambda(lambda x: x / 255) (inputs)  # rescale inputs\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1])\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\np5 = Conv2DTranspose(16, (8, 8), strides=(16,16), padding = 'same') (c5)\nu10 = concatenate([p5, c9], axis=3)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u10)\nc10 = Dropout(0.1) (c10)\nc10 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c10)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c10)\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[competitionMetric2])\n\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #data augmentation/ generation\n# data_gen_args = dict(#featurewise_center=True,\n#                  #featurewise_std_normalization=True,\n#                  #rotation_range=10,\n#                  #zca_whitening=True,\n#                  #width_shift_range=0.2,\n#                  #height_shift_range=0.2,\n#                  #zoom_range=0.2,\n#                  #horizontal_flip=True,\n#                  #vertical_flip=True\n#                 )\n# image_datagen = ImageDataGenerator(**data_gen_args)\n# mask_datagen = ImageDataGenerator(**data_gen_args)\n# # Provide the same seed and keyword arguments to the fit and flow methods\n# #seed = 10\n# #image_datagen.fit(X_train, augment=True#, seed=seed)\n# #mask_datagen.fit(Y_train, augment=True#, seed=seed)\n# image_generator = image_datagen.flow(\n#     X_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# mask_generator = mask_datagen.flow(\n#     Y_train,\n#     batch_size=4,\n#     #seed=42\n# )\n# # combine generators into one which yields image and masks\n# # train_generator = (pair for pair in zip(image_generator, mask_generator))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nearlystop = EarlyStopping(monitor='val_loss',patience=5, verbose=1, restore_best_weights=True)\n\nmodel.fit(train_gen,\n          epochs=EPOCHS, \n          validation_data=valid_gen,\n          callbacks=[earlystop],\n          max_queue_size=max_queue_size, \n          workers=workers, \n          use_multiprocessing=use_multiprocessing\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{},"cell_type":"markdown","source":"As `x_gen` is already formatted, we can directly make a prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESH = 0.65\n\npreds = model.predict(test_gen)[:, :, :, 0]\nmasked_preds = preds > THRESH\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we want to binarize the prediction to get a mask (as the input y_train was also a mask)"},{"metadata":{},"cell_type":"markdown","source":"* Let's visualise predictions ! "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rows = 3\nf, ax = plt.subplots(n_rows, 3, figsize=(14, 10))\n\nfor j, idx in enumerate([4,5,6]):\n    for k, kind in enumerate(['original', 'pred', 'masked_pred']):\n        if kind == 'original':\n            img = test_gen[idx]\n        elif kind == 'pred':\n            img = preds[idx]\n        elif kind == 'masked_pred':\n            masked_pred = preds[idx] > THRESH\n            img = masked_pred\n        ax[j, k].imshow(img)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems not too bad ! "},{"metadata":{},"cell_type":"markdown","source":"### Now the next step is to translate our masked predictions into several bouding boxes"},{"metadata":{},"cell_type":"markdown","source":"For the moment, I'll assign 1.0 confidence for every box"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_params_from_bbox(coords, scaling_factor=1):\n    xmin, ymin = coords[1] * scaling_factor, coords[0] * scaling_factor\n    w = (coords[3] - coords[1]) * scaling_factor\n    h = (coords[2] - coords[0]) * scaling_factor\n    \n    return xmin, ymin, w, h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Allows to extract bounding boxes from binary masks\nbboxes = list()\n\nfor j in range(masked_preds.shape[0]):\n    label_j = label(masked_preds[j, :, :]) \n    props = regionprops(label_j)   # that's were the job is done\n    bboxes.append(props)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we format the bboxes into the required format\noutput = dict()\n\nfor i in range(masked_preds.shape[0]):\n    bboxes_processed = [get_params_from_bbox(bb.bbox, scaling_factor=SC_FACTOR) for bb in bboxes[i]]\n    formated_boxes = ['1.0 ' + ' '.join(map(str, bb_m)) for bb_m in bboxes_processed]\n    \n    output[sample_sub[\"image_id\"][i]] = \" \".join(formated_boxes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub[\"PredictionString\"] = output.values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}